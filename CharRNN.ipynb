{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\l\\anaconda3\\envs\\aind-cv1\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt','r') as f:\n",
    "    text = f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c:i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  7,  0, 61,  1, 10, 62, 63, 23, 74, 74, 74, 64,  0, 61, 61, 35,\n",
       "       63, 42,  0, 69, 28, 18, 28, 10, 25, 63,  0, 62, 10, 63,  0, 18, 18,\n",
       "       63,  0, 18, 28, 29, 10, 39, 63, 10, 19, 10, 62, 35, 63, 72,  2,  7,\n",
       "        0, 61, 61, 35, 63, 42,  0, 69, 28, 18, 35, 63, 28, 25, 63, 72,  2,\n",
       "        7,  0, 61, 61, 35, 63, 28,  2, 63, 28,  1, 25, 63, 50, 49,  2, 74,\n",
       "       49,  0, 35, 59, 74, 74,  3, 19, 10, 62, 35,  1,  7, 28,  2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    arr = arr[:n_batches*characters_per_batch]\n",
    "    arr = arr.reshape((n_seqs,-1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        y[:, :-1], y[:,-1] = x[:,1:],x[:,0]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 5  7  0 61  1 10 62 63 23 74]\n",
      " [63  0 69 63  2 50  1 63 81 50]\n",
      " [19 28  2 59 74 74 38 17 10 25]\n",
      " [ 2 63 76 72 62 28  2 81 63  7]\n",
      " [63 28  1 63 28 25 57 63 25 28]\n",
      " [63 32  1 63 49  0 25 74 50  2]\n",
      " [ 7 10  2 63 24 50 69 10 63 42]\n",
      " [39 63 43 72  1 63  2 50 49 63]\n",
      " [ 1 63 28 25  2  8  1 59 63 44]\n",
      " [63 25  0 28 76 63  1 50 63  7]]\n",
      "\n",
      "y\n",
      " [[ 7  0 61  1 10 62 63 23 74 74]\n",
      " [ 0 69 63  2 50  1 63 81 50 28]\n",
      " [28  2 59 74 74 38 17 10 25 57]\n",
      " [63 76 72 62 28  2 81 63  7 28]\n",
      " [28  1 63 28 25 57 63 25 28 62]\n",
      " [32  1 63 49  0 25 74 50  2 18]\n",
      " [10  2 63 24 50 69 10 63 42 50]\n",
      " [63 43 72  1 63  2 50 49 63 25]\n",
      " [63 28 25  2  8  1 59 63 44  7]\n",
      " [25  0 28 76 63  1 50 63  7 10]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = keep_prob)\n",
    "        return drop\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    \n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    logits = tf.matmul(x,softmax_w) + softmax_b\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    \n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50,\n",
    "                lstm_size=128, num_layers=2, learning_rate=0.001,\n",
    "                grad_clip=5, sampling=False):\n",
    "        \n",
    "        if sampling==True:\n",
    "            batch_size, num_steps = 1,1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state = self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100        \n",
    "num_steps = 100         \n",
    "lstm_size = 512         \n",
    "num_layers = 2          \n",
    "learning_rate = 0.001   \n",
    "keep_prob = 0.5         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.4177...  10.7653 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.3276...  7.1861 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 3.8290...  7.2061 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 5.2100...  7.1581 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 4.3128...  7.2051 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.8016...  8.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.7036...  8.2479 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.5802...  8.1058 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.4868...  8.0893 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.4194...  8.0547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.3607...  8.0727 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.3515...  8.0967 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3387...  8.0768 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3484...  8.0487 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.3232...  8.6211 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.3145...  8.1478 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.2838...  8.1208 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.3083...  8.5851 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.2808...  8.2869 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2357...  8.1958 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2639...  8.1418 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2559...  9.1965 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2429...  9.9131 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.2364...  8.4650 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.2208...  8.0517 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.2386...  8.0757 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.2326...  8.3940 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.2078...  8.0447 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.2099...  8.0377 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2111...  8.0502 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2299...  8.0812 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.2034...  8.0617 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.1868...  8.0087 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.2116...  8.1058 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.1894...  9.2436 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.2022...  10.5235 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1765...  8.0422 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1735...  8.0252 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1633...  8.0617 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1713...  8.0287 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1689...  8.1168 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1764...  8.0977 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1618...  8.0297 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.1660...  8.0457 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1552...  8.0387 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1749...  8.0607 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1673...  8.0237 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.1781...  8.0307 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1762...  8.1293 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1673...  8.1318 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1603...  8.1008 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1512...  8.0567 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.1560...  8.1498 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.1440...  8.1298 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1592...  8.8743 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.1385...  9.4847 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.1437...  8.7572 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.1483...  8.0467 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1387...  8.0537 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1496...  8.0672 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.1450...  8.0937 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.1644...  8.0637 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1685...  8.3730 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1255...  8.1698 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1219...  8.2439 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1527...  8.0417 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1456...  8.0963 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.1015...  8.2298 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1264...  9.8189 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1419...  11.4581 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1328...  11.6483 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1521...  11.8950 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.1263...  10.1697 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 3.1351...  10.3128 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 3.1359...  9.9311 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 3.1438...  9.9671 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.1369...  10.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.1298...  8.8833 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.1194...  8.4745 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.1104...  8.3691 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.1111...  9.0034 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1240...  9.7589 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.1225...  9.2951 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.1139...  9.6403 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.0987...  8.8513 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.1056...  8.8763 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.0934...  9.8870 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 3.0980...  9.0694 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 3.2401...  11.1954 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 3.4413...  9.5113 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 3.3873...  9.0284 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.2609...  8.5090 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.1424...  9.8925 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.1240...  8.5170 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.1066...  8.6476 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.1155...  8.9924 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.1345...  8.3214 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.1189...  8.3684 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.1143...  10.0151 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.1132...  9.6489 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.1163...  9.6539 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.1095...  9.6519 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.1055...  9.7479 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.1048...  9.6969 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.0966...  9.6259 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.0939...  9.6699 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.0741...  9.5648 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.0810...  9.6163 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.0856...  9.6969 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 3.0470...  10.2343 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 3.0680...  8.3850 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 3.0779...  8.4160 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 3.0609...  8.7042 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 3.0482...  8.2569 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 3.0435...  8.6852 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 3.0364...  8.8593 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 3.0457...  8.2979 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 3.0608...  8.2188 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 3.0547...  8.2779 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 3.0262...  8.3489 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 3.0548...  8.2474 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 3.0307...  8.2554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 3.0211...  8.3589 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 3.0363...  8.3129 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 3.0031...  9.4918 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 2.9776...  10.3684 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 2.9925...  10.8027 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 2.9930...  11.4742 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 2.9768...  11.6188 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 2.9716...  11.5342 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 2.9681...  10.3979 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 2.9385...  8.7312 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 2.9544...  10.3954 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 2.9617...  8.4450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 2.9154...  8.2929 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 2.9271...  10.3714 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 2.9194...  10.4694 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 2.8984...  11.2470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 2.9119...  11.6243 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 2.8928...  8.9669 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 2.8865...  9.9050 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 2.8606...  10.3664 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 2.8620...  9.0630 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 2.8541...  8.2559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 2.8549...  8.3159 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 2.8411...  8.3314 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 2.8447...  8.3289 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 2.8505...  8.2529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 2.7986...  8.2469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.8018...  8.2944 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 2.8633...  8.8063 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 2.8951...  8.2288 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 2.8257...  8.4540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 2.8429...  8.3389 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 2.7789...  8.3739 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 2.7963...  8.3129 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 2.7635...  8.2484 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 2.7582...  8.2328 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 2.7623...  8.2739 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 2.7477...  8.5661 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 2.7481...  8.3089 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 2.7123...  8.3229 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 2.6922...  8.3449 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 2.7105...  8.5911 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 2.7077...  9.5628 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 2.6863...  9.0414 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 2.6910...  10.4073 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 2.6663...  9.4837 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 2.6771...  8.7162 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 2.6470...  8.1808 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 2.6565...  8.1908 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 2.6768...  8.2048 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 2.6734...  8.4670 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 2.6646...  8.8538 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 2.6390...  8.3960 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 2.6247...  8.3669 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 2.6074...  8.4240 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 2.5944...  8.5026 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 2.5864...  8.9444 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 2.5665...  8.6481 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 2.5835...  9.0770 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 2.5797...  9.6719 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 2.5574...  8.3839 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 2.5763...  8.3244 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 2.5941...  8.2294 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 2.5749...  8.5200 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 2.5372...  8.2629 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 2.5313...  8.3239 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 2.5314...  8.2749 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 2.5306...  8.4870 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 2.5341...  8.3539 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 2.5072...  8.2409 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 2.5377...  8.4600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 2.5090...  11.1699 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 2.5190...  10.8667 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 2.4994...  10.1782 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 2.4989...  8.3789 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 2.4861...  8.3920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 2.5767...  8.2178 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4866...  8.1878 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 2.4879...  9.6864 sec/batch\n",
      "Epoch: 2/20...  Training Step: 202...  Training loss: 2.5047...  8.8483 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 203...  Training loss: 2.4911...  8.5561 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 2.4902...  8.0347 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 2.4926...  8.3099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 2.4986...  8.2569 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 2.5038...  8.3870 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 2.4801...  10.4524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 2.4616...  9.6444 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 2.4798...  10.5725 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 2.4721...  11.0464 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 2.4972...  10.7026 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 2.4749...  13.1053 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 2.4675...  10.0061 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 2.4665...  11.3631 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 2.4960...  11.2950 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 2.4694...  11.9362 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 2.4329...  8.5461 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 2.4436...  8.2259 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 2.4782...  9.1065 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 2.4434...  9.0744 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 2.4325...  8.6371 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 2.4343...  8.2038 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 2.4385...  8.1758 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 2.4294...  8.1688 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 2.4254...  8.6221 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 2.4383...  8.4040 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 2.4353...  8.3840 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 2.4459...  8.2519 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 2.4122...  8.2744 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 2.4039...  8.2308 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 2.4285...  8.2018 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 2.4034...  8.2299 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 2.4183...  8.2804 sec/batch\n",
      "Epoch: 2/20...  Training Step: 235...  Training loss: 2.4128...  8.4630 sec/batch\n",
      "Epoch: 2/20...  Training Step: 236...  Training loss: 2.3771...  8.2909 sec/batch\n",
      "Epoch: 2/20...  Training Step: 237...  Training loss: 2.3925...  8.3640 sec/batch\n",
      "Epoch: 2/20...  Training Step: 238...  Training loss: 2.3894...  8.1788 sec/batch\n",
      "Epoch: 2/20...  Training Step: 239...  Training loss: 2.3762...  8.1778 sec/batch\n",
      "Epoch: 2/20...  Training Step: 240...  Training loss: 2.3804...  8.2308 sec/batch\n",
      "Epoch: 2/20...  Training Step: 241...  Training loss: 2.3740...  8.1813 sec/batch\n",
      "Epoch: 2/20...  Training Step: 242...  Training loss: 2.3687...  8.2819 sec/batch\n",
      "Epoch: 2/20...  Training Step: 243...  Training loss: 2.3816...  9.1895 sec/batch\n",
      "Epoch: 2/20...  Training Step: 244...  Training loss: 2.3379...  9.7675 sec/batch\n",
      "Epoch: 2/20...  Training Step: 245...  Training loss: 2.3993...  8.8983 sec/batch\n",
      "Epoch: 2/20...  Training Step: 246...  Training loss: 2.3639...  10.3493 sec/batch\n",
      "Epoch: 2/20...  Training Step: 247...  Training loss: 2.3680...  8.8888 sec/batch\n",
      "Epoch: 2/20...  Training Step: 248...  Training loss: 2.3900...  8.3019 sec/batch\n",
      "Epoch: 2/20...  Training Step: 249...  Training loss: 2.3543...  8.4200 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.3788...  8.6692 sec/batch\n",
      "Epoch: 2/20...  Training Step: 251...  Training loss: 2.3601...  8.1898 sec/batch\n",
      "Epoch: 2/20...  Training Step: 252...  Training loss: 2.3561...  8.1868 sec/batch\n",
      "Epoch: 2/20...  Training Step: 253...  Training loss: 2.3517...  11.0308 sec/batch\n",
      "Epoch: 2/20...  Training Step: 254...  Training loss: 2.3634...  10.9248 sec/batch\n",
      "Epoch: 2/20...  Training Step: 255...  Training loss: 2.3570...  9.2841 sec/batch\n",
      "Epoch: 2/20...  Training Step: 256...  Training loss: 2.3490...  9.2716 sec/batch\n",
      "Epoch: 2/20...  Training Step: 257...  Training loss: 2.3381...  9.2251 sec/batch\n",
      "Epoch: 2/20...  Training Step: 258...  Training loss: 2.3743...  8.5601 sec/batch\n",
      "Epoch: 2/20...  Training Step: 259...  Training loss: 2.3523...  8.4580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 260...  Training loss: 2.3523...  9.3306 sec/batch\n",
      "Epoch: 2/20...  Training Step: 261...  Training loss: 2.3737...  9.0249 sec/batch\n",
      "Epoch: 2/20...  Training Step: 262...  Training loss: 2.3437...  9.0404 sec/batch\n",
      "Epoch: 2/20...  Training Step: 263...  Training loss: 2.3357...  8.3199 sec/batch\n",
      "Epoch: 2/20...  Training Step: 264...  Training loss: 2.3514...  8.3514 sec/batch\n",
      "Epoch: 2/20...  Training Step: 265...  Training loss: 2.3425...  10.1582 sec/batch\n",
      "Epoch: 2/20...  Training Step: 266...  Training loss: 2.3129...  8.6882 sec/batch\n",
      "Epoch: 2/20...  Training Step: 267...  Training loss: 2.3130...  11.0944 sec/batch\n",
      "Epoch: 2/20...  Training Step: 268...  Training loss: 2.3281...  9.8770 sec/batch\n",
      "Epoch: 2/20...  Training Step: 269...  Training loss: 2.3454...  8.2909 sec/batch\n",
      "Epoch: 2/20...  Training Step: 270...  Training loss: 2.3414...  8.4300 sec/batch\n",
      "Epoch: 2/20...  Training Step: 271...  Training loss: 2.3335...  8.7777 sec/batch\n",
      "Epoch: 2/20...  Training Step: 272...  Training loss: 2.3085...  8.6061 sec/batch\n",
      "Epoch: 2/20...  Training Step: 273...  Training loss: 2.3180...  8.5561 sec/batch\n",
      "Epoch: 2/20...  Training Step: 274...  Training loss: 2.3648...  8.9314 sec/batch\n",
      "Epoch: 2/20...  Training Step: 275...  Training loss: 2.3192...  8.6121 sec/batch\n",
      "Epoch: 2/20...  Training Step: 276...  Training loss: 2.3305...  10.3363 sec/batch\n",
      "Epoch: 2/20...  Training Step: 277...  Training loss: 2.2936...  9.7014 sec/batch\n",
      "Epoch: 2/20...  Training Step: 278...  Training loss: 2.2956...  9.8530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 279...  Training loss: 2.2956...  8.3820 sec/batch\n",
      "Epoch: 2/20...  Training Step: 280...  Training loss: 2.3252...  8.3645 sec/batch\n",
      "Epoch: 2/20...  Training Step: 281...  Training loss: 2.2875...  8.2298 sec/batch\n",
      "Epoch: 2/20...  Training Step: 282...  Training loss: 2.2757...  8.2278 sec/batch\n",
      "Epoch: 2/20...  Training Step: 283...  Training loss: 2.2673...  8.4780 sec/batch\n",
      "Epoch: 2/20...  Training Step: 284...  Training loss: 2.2858...  8.2564 sec/batch\n",
      "Epoch: 2/20...  Training Step: 285...  Training loss: 2.2984...  8.8893 sec/batch\n",
      "Epoch: 2/20...  Training Step: 286...  Training loss: 2.2884...  8.6071 sec/batch\n",
      "Epoch: 2/20...  Training Step: 287...  Training loss: 2.2674...  8.2759 sec/batch\n",
      "Epoch: 2/20...  Training Step: 288...  Training loss: 2.3013...  8.3319 sec/batch\n",
      "Epoch: 2/20...  Training Step: 289...  Training loss: 2.2724...  8.7392 sec/batch\n",
      "Epoch: 2/20...  Training Step: 290...  Training loss: 2.2935...  8.6952 sec/batch\n",
      "Epoch: 2/20...  Training Step: 291...  Training loss: 2.2604...  8.5711 sec/batch\n",
      "Epoch: 2/20...  Training Step: 292...  Training loss: 2.2652...  9.0084 sec/batch\n",
      "Epoch: 2/20...  Training Step: 293...  Training loss: 2.2560...  8.2989 sec/batch\n",
      "Epoch: 2/20...  Training Step: 294...  Training loss: 2.2582...  8.5951 sec/batch\n",
      "Epoch: 2/20...  Training Step: 295...  Training loss: 2.2707...  8.3780 sec/batch\n",
      "Epoch: 2/20...  Training Step: 296...  Training loss: 2.2598...  8.6902 sec/batch\n",
      "Epoch: 2/20...  Training Step: 297...  Training loss: 2.2550...  9.6448 sec/batch\n",
      "Epoch: 2/20...  Training Step: 298...  Training loss: 2.2450...  8.2264 sec/batch\n",
      "Epoch: 2/20...  Training Step: 299...  Training loss: 2.2856...  8.2689 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2650...  8.2018 sec/batch\n",
      "Epoch: 2/20...  Training Step: 301...  Training loss: 2.2395...  8.3049 sec/batch\n",
      "Epoch: 2/20...  Training Step: 302...  Training loss: 2.2469...  8.3089 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 303...  Training loss: 2.2452...  8.4060 sec/batch\n",
      "Epoch: 2/20...  Training Step: 304...  Training loss: 2.2521...  8.2789 sec/batch\n",
      "Epoch: 2/20...  Training Step: 305...  Training loss: 2.2499...  8.2023 sec/batch\n",
      "Epoch: 2/20...  Training Step: 306...  Training loss: 2.2770...  8.2709 sec/batch\n",
      "Epoch: 2/20...  Training Step: 307...  Training loss: 2.2755...  8.2028 sec/batch\n",
      "Epoch: 2/20...  Training Step: 308...  Training loss: 2.2443...  8.2779 sec/batch\n",
      "Epoch: 2/20...  Training Step: 309...  Training loss: 2.2532...  8.4600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 310...  Training loss: 2.2598...  8.3689 sec/batch\n",
      "Epoch: 2/20...  Training Step: 311...  Training loss: 2.2390...  8.3099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 312...  Training loss: 2.2310...  8.2654 sec/batch\n",
      "Epoch: 2/20...  Training Step: 313...  Training loss: 2.2343...  8.2208 sec/batch\n",
      "Epoch: 2/20...  Training Step: 314...  Training loss: 2.2012...  8.5180 sec/batch\n",
      "Epoch: 2/20...  Training Step: 315...  Training loss: 2.2317...  8.3029 sec/batch\n",
      "Epoch: 2/20...  Training Step: 316...  Training loss: 2.2368...  8.3274 sec/batch\n",
      "Epoch: 2/20...  Training Step: 317...  Training loss: 2.2565...  8.3749 sec/batch\n",
      "Epoch: 2/20...  Training Step: 318...  Training loss: 2.2382...  8.2859 sec/batch\n",
      "Epoch: 2/20...  Training Step: 319...  Training loss: 2.2580...  8.2459 sec/batch\n",
      "Epoch: 2/20...  Training Step: 320...  Training loss: 2.2088...  8.2599 sec/batch\n",
      "Epoch: 2/20...  Training Step: 321...  Training loss: 2.2121...  8.2869 sec/batch\n",
      "Epoch: 2/20...  Training Step: 322...  Training loss: 2.2504...  8.1848 sec/batch\n",
      "Epoch: 2/20...  Training Step: 323...  Training loss: 2.2228...  8.2444 sec/batch\n",
      "Epoch: 2/20...  Training Step: 324...  Training loss: 2.1997...  8.3329 sec/batch\n",
      "Epoch: 2/20...  Training Step: 325...  Training loss: 2.2429...  8.3099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 326...  Training loss: 2.2307...  8.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 327...  Training loss: 2.2254...  8.3514 sec/batch\n",
      "Epoch: 2/20...  Training Step: 328...  Training loss: 2.2145...  8.4030 sec/batch\n",
      "Epoch: 2/20...  Training Step: 329...  Training loss: 2.2058...  8.3109 sec/batch\n",
      "Epoch: 2/20...  Training Step: 330...  Training loss: 2.1945...  8.6487 sec/batch\n",
      "Epoch: 2/20...  Training Step: 331...  Training loss: 2.2170...  8.3439 sec/batch\n",
      "Epoch: 2/20...  Training Step: 332...  Training loss: 2.2284...  8.6041 sec/batch\n",
      "Epoch: 2/20...  Training Step: 333...  Training loss: 2.2128...  8.3089 sec/batch\n",
      "Epoch: 2/20...  Training Step: 334...  Training loss: 2.2141...  8.3870 sec/batch\n",
      "Epoch: 2/20...  Training Step: 335...  Training loss: 2.2056...  8.2849 sec/batch\n",
      "Epoch: 2/20...  Training Step: 336...  Training loss: 2.2087...  8.2509 sec/batch\n",
      "Epoch: 2/20...  Training Step: 337...  Training loss: 2.2433...  8.2509 sec/batch\n",
      "Epoch: 2/20...  Training Step: 338...  Training loss: 2.1960...  8.2624 sec/batch\n",
      "Epoch: 2/20...  Training Step: 339...  Training loss: 2.2213...  8.3820 sec/batch\n",
      "Epoch: 2/20...  Training Step: 340...  Training loss: 2.1920...  8.3880 sec/batch\n",
      "Epoch: 2/20...  Training Step: 341...  Training loss: 2.1991...  8.2039 sec/batch\n",
      "Epoch: 2/20...  Training Step: 342...  Training loss: 2.1894...  8.2759 sec/batch\n",
      "Epoch: 2/20...  Training Step: 343...  Training loss: 2.1942...  8.2368 sec/batch\n",
      "Epoch: 2/20...  Training Step: 344...  Training loss: 2.2252...  8.2609 sec/batch\n",
      "Epoch: 2/20...  Training Step: 345...  Training loss: 2.2115...  8.2794 sec/batch\n",
      "Epoch: 2/20...  Training Step: 346...  Training loss: 2.2173...  8.5511 sec/batch\n",
      "Epoch: 2/20...  Training Step: 347...  Training loss: 2.1929...  8.4000 sec/batch\n",
      "Epoch: 2/20...  Training Step: 348...  Training loss: 2.1755...  8.2153 sec/batch\n",
      "Epoch: 2/20...  Training Step: 349...  Training loss: 2.2095...  8.2274 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.2259...  8.3179 sec/batch\n",
      "Epoch: 2/20...  Training Step: 351...  Training loss: 2.1958...  8.3079 sec/batch\n",
      "Epoch: 2/20...  Training Step: 352...  Training loss: 2.2017...  8.2619 sec/batch\n",
      "Epoch: 2/20...  Training Step: 353...  Training loss: 2.1765...  8.3009 sec/batch\n",
      "Epoch: 2/20...  Training Step: 354...  Training loss: 2.1733...  8.2919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 355...  Training loss: 2.1794...  8.2258 sec/batch\n",
      "Epoch: 2/20...  Training Step: 356...  Training loss: 2.1734...  8.2844 sec/batch\n",
      "Epoch: 2/20...  Training Step: 357...  Training loss: 2.1421...  8.7692 sec/batch\n",
      "Epoch: 2/20...  Training Step: 358...  Training loss: 2.2244...  8.1878 sec/batch\n",
      "Epoch: 2/20...  Training Step: 359...  Training loss: 2.1869...  8.2249 sec/batch\n",
      "Epoch: 2/20...  Training Step: 360...  Training loss: 2.1645...  8.1918 sec/batch\n",
      "Epoch: 2/20...  Training Step: 361...  Training loss: 2.1716...  8.4760 sec/batch\n",
      "Epoch: 2/20...  Training Step: 362...  Training loss: 2.1774...  8.6021 sec/batch\n",
      "Epoch: 2/20...  Training Step: 363...  Training loss: 2.1733...  8.3104 sec/batch\n",
      "Epoch: 2/20...  Training Step: 364...  Training loss: 2.1589...  8.2949 sec/batch\n",
      "Epoch: 2/20...  Training Step: 365...  Training loss: 2.1718...  8.2228 sec/batch\n",
      "Epoch: 2/20...  Training Step: 366...  Training loss: 2.1797...  8.5596 sec/batch\n",
      "Epoch: 2/20...  Training Step: 367...  Training loss: 2.1569...  8.2048 sec/batch\n",
      "Epoch: 2/20...  Training Step: 368...  Training loss: 2.1482...  8.3349 sec/batch\n",
      "Epoch: 2/20...  Training Step: 369...  Training loss: 2.1471...  8.3339 sec/batch\n",
      "Epoch: 2/20...  Training Step: 370...  Training loss: 2.1628...  8.2534 sec/batch\n",
      "Epoch: 2/20...  Training Step: 371...  Training loss: 2.1777...  8.2579 sec/batch\n",
      "Epoch: 2/20...  Training Step: 372...  Training loss: 2.1723...  8.2909 sec/batch\n",
      "Epoch: 2/20...  Training Step: 373...  Training loss: 2.1615...  8.2529 sec/batch\n",
      "Epoch: 2/20...  Training Step: 374...  Training loss: 2.1493...  8.3074 sec/batch\n",
      "Epoch: 2/20...  Training Step: 375...  Training loss: 2.1387...  8.5881 sec/batch\n",
      "Epoch: 2/20...  Training Step: 376...  Training loss: 2.1529...  9.0204 sec/batch\n",
      "Epoch: 2/20...  Training Step: 377...  Training loss: 2.1231...  8.3850 sec/batch\n",
      "Epoch: 2/20...  Training Step: 378...  Training loss: 2.1052...  8.2659 sec/batch\n",
      "Epoch: 2/20...  Training Step: 379...  Training loss: 2.1233...  8.2388 sec/batch\n",
      "Epoch: 2/20...  Training Step: 380...  Training loss: 2.1430...  8.2569 sec/batch\n",
      "Epoch: 2/20...  Training Step: 381...  Training loss: 2.1363...  8.2084 sec/batch\n",
      "Epoch: 2/20...  Training Step: 382...  Training loss: 2.1674...  8.1988 sec/batch\n",
      "Epoch: 2/20...  Training Step: 383...  Training loss: 2.1496...  8.2899 sec/batch\n",
      "Epoch: 2/20...  Training Step: 384...  Training loss: 2.1254...  8.3509 sec/batch\n",
      "Epoch: 2/20...  Training Step: 385...  Training loss: 2.1125...  8.2188 sec/batch\n",
      "Epoch: 2/20...  Training Step: 386...  Training loss: 2.1136...  8.2829 sec/batch\n",
      "Epoch: 2/20...  Training Step: 387...  Training loss: 2.1118...  8.2138 sec/batch\n",
      "Epoch: 2/20...  Training Step: 388...  Training loss: 2.1246...  8.2329 sec/batch\n",
      "Epoch: 2/20...  Training Step: 389...  Training loss: 2.1424...  8.3609 sec/batch\n",
      "Epoch: 2/20...  Training Step: 390...  Training loss: 2.1007...  8.4350 sec/batch\n",
      "Epoch: 2/20...  Training Step: 391...  Training loss: 2.1302...  8.2719 sec/batch\n",
      "Epoch: 2/20...  Training Step: 392...  Training loss: 2.1185...  8.3710 sec/batch\n",
      "Epoch: 2/20...  Training Step: 393...  Training loss: 2.1016...  8.3459 sec/batch\n",
      "Epoch: 2/20...  Training Step: 394...  Training loss: 2.1226...  8.2168 sec/batch\n",
      "Epoch: 2/20...  Training Step: 395...  Training loss: 2.1135...  8.2249 sec/batch\n",
      "Epoch: 2/20...  Training Step: 396...  Training loss: 2.1116...  8.1938 sec/batch\n",
      "Epoch: 3/20...  Training Step: 397...  Training loss: 2.1967...  8.1938 sec/batch\n",
      "Epoch: 3/20...  Training Step: 398...  Training loss: 2.0815...  8.3149 sec/batch\n",
      "Epoch: 3/20...  Training Step: 399...  Training loss: 2.0961...  8.3835 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.1113...  8.3699 sec/batch\n",
      "Epoch: 3/20...  Training Step: 401...  Training loss: 2.1049...  11.9510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 402...  Training loss: 2.0849...  9.4727 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 403...  Training loss: 2.1061...  12.5429 sec/batch\n",
      "Epoch: 3/20...  Training Step: 404...  Training loss: 2.1027...  9.4667 sec/batch\n",
      "Epoch: 3/20...  Training Step: 405...  Training loss: 2.1291...  12.0446 sec/batch\n",
      "Epoch: 3/20...  Training Step: 406...  Training loss: 2.1014...  10.0491 sec/batch\n",
      "Epoch: 3/20...  Training Step: 407...  Training loss: 2.0851...  11.9735 sec/batch\n",
      "Epoch: 3/20...  Training Step: 408...  Training loss: 2.0713...  9.9841 sec/batch\n",
      "Epoch: 3/20...  Training Step: 409...  Training loss: 2.0994...  9.4737 sec/batch\n",
      "Epoch: 3/20...  Training Step: 410...  Training loss: 2.1285...  12.3983 sec/batch\n",
      "Epoch: 3/20...  Training Step: 411...  Training loss: 2.0788...  10.4074 sec/batch\n",
      "Epoch: 3/20...  Training Step: 412...  Training loss: 2.0713...  9.5538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 413...  Training loss: 2.0845...  9.9756 sec/batch\n",
      "Epoch: 3/20...  Training Step: 414...  Training loss: 2.1260...  10.1052 sec/batch\n",
      "Epoch: 3/20...  Training Step: 415...  Training loss: 2.0879...  9.5338 sec/batch\n",
      "Epoch: 3/20...  Training Step: 416...  Training loss: 2.0783...  9.8265 sec/batch\n",
      "Epoch: 3/20...  Training Step: 417...  Training loss: 2.0792...  9.4907 sec/batch\n",
      "Epoch: 3/20...  Training Step: 418...  Training loss: 2.1238...  9.9451 sec/batch\n",
      "Epoch: 3/20...  Training Step: 419...  Training loss: 2.0798...  12.0150 sec/batch\n",
      "Epoch: 3/20...  Training Step: 420...  Training loss: 2.0798...  9.5818 sec/batch\n",
      "Epoch: 3/20...  Training Step: 421...  Training loss: 2.0817...  9.8760 sec/batch\n",
      "Epoch: 3/20...  Training Step: 422...  Training loss: 2.0608...  9.5523 sec/batch\n",
      "Epoch: 3/20...  Training Step: 423...  Training loss: 2.0666...  10.0798 sec/batch\n",
      "Epoch: 3/20...  Training Step: 424...  Training loss: 2.0812...  11.5272 sec/batch\n",
      "Epoch: 3/20...  Training Step: 425...  Training loss: 2.1009...  9.8135 sec/batch\n",
      "Epoch: 3/20...  Training Step: 426...  Training loss: 2.0807...  9.6579 sec/batch\n",
      "Epoch: 3/20...  Training Step: 427...  Training loss: 2.0765...  9.8040 sec/batch\n",
      "Epoch: 3/20...  Training Step: 428...  Training loss: 2.0448...  11.1039 sec/batch\n",
      "Epoch: 3/20...  Training Step: 429...  Training loss: 2.0742...  10.7126 sec/batch\n",
      "Epoch: 3/20...  Training Step: 430...  Training loss: 2.0958...  9.4687 sec/batch\n",
      "Epoch: 3/20...  Training Step: 431...  Training loss: 2.0662...  12.5549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 432...  Training loss: 2.0646...  9.4827 sec/batch\n",
      "Epoch: 3/20...  Training Step: 433...  Training loss: 2.0605...  9.8300 sec/batch\n",
      "Epoch: 3/20...  Training Step: 434...  Training loss: 2.0264...  9.5208 sec/batch\n",
      "Epoch: 3/20...  Training Step: 435...  Training loss: 2.0230...  9.4937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 436...  Training loss: 2.0319...  9.8070 sec/batch\n",
      "Epoch: 3/20...  Training Step: 437...  Training loss: 2.0397...  9.5443 sec/batch\n",
      "Epoch: 3/20...  Training Step: 438...  Training loss: 2.0605...  10.0752 sec/batch\n",
      "Epoch: 3/20...  Training Step: 439...  Training loss: 2.0336...  9.5288 sec/batch\n",
      "Epoch: 3/20...  Training Step: 440...  Training loss: 2.0291...  9.5733 sec/batch\n",
      "Epoch: 3/20...  Training Step: 441...  Training loss: 2.0558...  12.0025 sec/batch\n",
      "Epoch: 3/20...  Training Step: 442...  Training loss: 2.0030...  9.4637 sec/batch\n",
      "Epoch: 3/20...  Training Step: 443...  Training loss: 2.0607...  10.2938 sec/batch\n",
      "Epoch: 3/20...  Training Step: 444...  Training loss: 2.0345...  12.2337 sec/batch\n",
      "Epoch: 3/20...  Training Step: 445...  Training loss: 2.0389...  9.5463 sec/batch\n",
      "Epoch: 3/20...  Training Step: 446...  Training loss: 2.0827...  12.4859 sec/batch\n",
      "Epoch: 3/20...  Training Step: 447...  Training loss: 2.0171...  9.5027 sec/batch\n",
      "Epoch: 3/20...  Training Step: 448...  Training loss: 2.0863...  12.0336 sec/batch\n",
      "Epoch: 3/20...  Training Step: 449...  Training loss: 2.0322...  10.0661 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 2.0383...  11.7624 sec/batch\n",
      "Epoch: 3/20...  Training Step: 451...  Training loss: 2.0344...  11.4542 sec/batch\n",
      "Epoch: 3/20...  Training Step: 452...  Training loss: 2.0456...  10.6486 sec/batch\n",
      "Epoch: 3/20...  Training Step: 453...  Training loss: 2.0475...  9.6284 sec/batch\n",
      "Epoch: 3/20...  Training Step: 454...  Training loss: 2.0323...  11.5407 sec/batch\n",
      "Epoch: 3/20...  Training Step: 455...  Training loss: 2.0192...  8.8367 sec/batch\n",
      "Epoch: 3/20...  Training Step: 456...  Training loss: 2.0641...  12.2827 sec/batch\n",
      "Epoch: 3/20...  Training Step: 457...  Training loss: 2.0351...  9.9466 sec/batch\n",
      "Epoch: 3/20...  Training Step: 458...  Training loss: 2.0713...  13.5299 sec/batch\n",
      "Epoch: 3/20...  Training Step: 459...  Training loss: 2.0646...  11.7248 sec/batch\n",
      "Epoch: 3/20...  Training Step: 460...  Training loss: 2.0428...  14.2606 sec/batch\n",
      "Epoch: 3/20...  Training Step: 461...  Training loss: 2.0198...  8.6942 sec/batch\n",
      "Epoch: 3/20...  Training Step: 462...  Training loss: 2.0586...  9.4192 sec/batch\n",
      "Epoch: 3/20...  Training Step: 463...  Training loss: 2.0357...  8.8303 sec/batch\n",
      "Epoch: 3/20...  Training Step: 464...  Training loss: 2.0064...  8.6371 sec/batch\n",
      "Epoch: 3/20...  Training Step: 465...  Training loss: 2.0154...  8.3159 sec/batch\n",
      "Epoch: 3/20...  Training Step: 466...  Training loss: 2.0306...  8.3179 sec/batch\n",
      "Epoch: 3/20...  Training Step: 467...  Training loss: 2.0531...  8.1868 sec/batch\n",
      "Epoch: 3/20...  Training Step: 468...  Training loss: 2.0277...  8.1893 sec/batch\n",
      "Epoch: 3/20...  Training Step: 469...  Training loss: 2.0423...  8.1678 sec/batch\n",
      "Epoch: 3/20...  Training Step: 470...  Training loss: 2.0081...  8.3629 sec/batch\n",
      "Epoch: 3/20...  Training Step: 471...  Training loss: 2.0137...  8.2158 sec/batch\n",
      "Epoch: 3/20...  Training Step: 472...  Training loss: 2.0570...  8.3489 sec/batch\n",
      "Epoch: 3/20...  Training Step: 473...  Training loss: 2.0202...  8.3779 sec/batch\n",
      "Epoch: 3/20...  Training Step: 474...  Training loss: 2.0260...  8.4120 sec/batch\n",
      "Epoch: 3/20...  Training Step: 475...  Training loss: 1.9832...  8.2038 sec/batch\n",
      "Epoch: 3/20...  Training Step: 476...  Training loss: 2.0043...  8.1888 sec/batch\n",
      "Epoch: 3/20...  Training Step: 477...  Training loss: 1.9796...  8.2389 sec/batch\n",
      "Epoch: 3/20...  Training Step: 478...  Training loss: 2.0290...  8.1768 sec/batch\n",
      "Epoch: 3/20...  Training Step: 479...  Training loss: 1.9803...  8.2354 sec/batch\n",
      "Epoch: 3/20...  Training Step: 480...  Training loss: 2.0003...  8.3059 sec/batch\n",
      "Epoch: 3/20...  Training Step: 481...  Training loss: 1.9800...  8.2839 sec/batch\n",
      "Epoch: 3/20...  Training Step: 482...  Training loss: 1.9927...  8.1808 sec/batch\n",
      "Epoch: 3/20...  Training Step: 483...  Training loss: 2.0065...  8.2674 sec/batch\n",
      "Epoch: 3/20...  Training Step: 484...  Training loss: 1.9841...  8.7842 sec/batch\n",
      "Epoch: 3/20...  Training Step: 485...  Training loss: 1.9693...  8.2719 sec/batch\n",
      "Epoch: 3/20...  Training Step: 486...  Training loss: 2.0114...  8.1958 sec/batch\n",
      "Epoch: 3/20...  Training Step: 487...  Training loss: 1.9819...  8.3174 sec/batch\n",
      "Epoch: 3/20...  Training Step: 488...  Training loss: 1.9935...  8.5120 sec/batch\n",
      "Epoch: 3/20...  Training Step: 489...  Training loss: 1.9629...  8.1998 sec/batch\n",
      "Epoch: 3/20...  Training Step: 490...  Training loss: 1.9780...  8.2103 sec/batch\n",
      "Epoch: 3/20...  Training Step: 491...  Training loss: 1.9753...  8.1668 sec/batch\n",
      "Epoch: 3/20...  Training Step: 492...  Training loss: 1.9906...  8.2108 sec/batch\n",
      "Epoch: 3/20...  Training Step: 493...  Training loss: 1.9936...  8.5911 sec/batch\n",
      "Epoch: 3/20...  Training Step: 494...  Training loss: 1.9700...  8.6251 sec/batch\n",
      "Epoch: 3/20...  Training Step: 495...  Training loss: 1.9697...  8.3890 sec/batch\n",
      "Epoch: 3/20...  Training Step: 496...  Training loss: 1.9458...  8.3479 sec/batch\n",
      "Epoch: 3/20...  Training Step: 497...  Training loss: 1.9951...  8.2529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 498...  Training loss: 1.9921...  8.2529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 499...  Training loss: 1.9800...  8.2459 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9699...  8.2068 sec/batch\n",
      "Epoch: 3/20...  Training Step: 501...  Training loss: 1.9734...  8.2789 sec/batch\n",
      "Epoch: 3/20...  Training Step: 502...  Training loss: 1.9740...  8.3279 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 503...  Training loss: 1.9764...  8.4380 sec/batch\n",
      "Epoch: 3/20...  Training Step: 504...  Training loss: 1.9915...  8.2529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 505...  Training loss: 1.9908...  8.2424 sec/batch\n",
      "Epoch: 3/20...  Training Step: 506...  Training loss: 1.9866...  8.2979 sec/batch\n",
      "Epoch: 3/20...  Training Step: 507...  Training loss: 1.9688...  8.1708 sec/batch\n",
      "Epoch: 3/20...  Training Step: 508...  Training loss: 1.9632...  8.2008 sec/batch\n",
      "Epoch: 3/20...  Training Step: 509...  Training loss: 1.9709...  8.3139 sec/batch\n",
      "Epoch: 3/20...  Training Step: 510...  Training loss: 1.9668...  8.3459 sec/batch\n",
      "Epoch: 3/20...  Training Step: 511...  Training loss: 1.9481...  8.1968 sec/batch\n",
      "Epoch: 3/20...  Training Step: 512...  Training loss: 1.9315...  8.2239 sec/batch\n",
      "Epoch: 3/20...  Training Step: 513...  Training loss: 1.9729...  8.2078 sec/batch\n",
      "Epoch: 3/20...  Training Step: 514...  Training loss: 1.9596...  8.2318 sec/batch\n",
      "Epoch: 3/20...  Training Step: 515...  Training loss: 1.9752...  8.2263 sec/batch\n",
      "Epoch: 3/20...  Training Step: 516...  Training loss: 1.9656...  8.1983 sec/batch\n",
      "Epoch: 3/20...  Training Step: 517...  Training loss: 1.9820...  8.4360 sec/batch\n",
      "Epoch: 3/20...  Training Step: 518...  Training loss: 1.9439...  8.3149 sec/batch\n",
      "Epoch: 3/20...  Training Step: 519...  Training loss: 1.9471...  8.1993 sec/batch\n",
      "Epoch: 3/20...  Training Step: 520...  Training loss: 1.9955...  8.1908 sec/batch\n",
      "Epoch: 3/20...  Training Step: 521...  Training loss: 1.9602...  8.2318 sec/batch\n",
      "Epoch: 3/20...  Training Step: 522...  Training loss: 1.9179...  8.1448 sec/batch\n",
      "Epoch: 3/20...  Training Step: 523...  Training loss: 1.9694...  8.2083 sec/batch\n",
      "Epoch: 3/20...  Training Step: 524...  Training loss: 1.9820...  8.2489 sec/batch\n",
      "Epoch: 3/20...  Training Step: 525...  Training loss: 1.9593...  8.2929 sec/batch\n",
      "Epoch: 3/20...  Training Step: 526...  Training loss: 1.9589...  8.1818 sec/batch\n",
      "Epoch: 3/20...  Training Step: 527...  Training loss: 1.9253...  8.2188 sec/batch\n",
      "Epoch: 3/20...  Training Step: 528...  Training loss: 1.9359...  8.1838 sec/batch\n",
      "Epoch: 3/20...  Training Step: 529...  Training loss: 1.9634...  8.1838 sec/batch\n",
      "Epoch: 3/20...  Training Step: 530...  Training loss: 1.9541...  8.9168 sec/batch\n",
      "Epoch: 3/20...  Training Step: 531...  Training loss: 1.9544...  8.3840 sec/batch\n",
      "Epoch: 3/20...  Training Step: 532...  Training loss: 1.9617...  9.7880 sec/batch\n",
      "Epoch: 3/20...  Training Step: 533...  Training loss: 1.9609...  9.9236 sec/batch\n",
      "Epoch: 3/20...  Training Step: 534...  Training loss: 1.9559...  10.4214 sec/batch\n",
      "Epoch: 3/20...  Training Step: 535...  Training loss: 1.9854...  9.0214 sec/batch\n",
      "Epoch: 3/20...  Training Step: 536...  Training loss: 1.9451...  9.7219 sec/batch\n",
      "Epoch: 3/20...  Training Step: 537...  Training loss: 1.9751...  9.0204 sec/batch\n",
      "Epoch: 3/20...  Training Step: 538...  Training loss: 1.9472...  9.6468 sec/batch\n",
      "Epoch: 3/20...  Training Step: 539...  Training loss: 1.9441...  10.3193 sec/batch\n",
      "Epoch: 3/20...  Training Step: 540...  Training loss: 1.9501...  9.2756 sec/batch\n",
      "Epoch: 3/20...  Training Step: 541...  Training loss: 1.9273...  9.4107 sec/batch\n",
      "Epoch: 3/20...  Training Step: 542...  Training loss: 1.9579...  8.9073 sec/batch\n",
      "Epoch: 3/20...  Training Step: 543...  Training loss: 1.9629...  10.0582 sec/batch\n",
      "Epoch: 3/20...  Training Step: 544...  Training loss: 1.9749...  9.6188 sec/batch\n",
      "Epoch: 3/20...  Training Step: 545...  Training loss: 1.9478...  8.2979 sec/batch\n",
      "Epoch: 3/20...  Training Step: 546...  Training loss: 1.9409...  8.5416 sec/batch\n",
      "Epoch: 3/20...  Training Step: 547...  Training loss: 1.9337...  8.6331 sec/batch\n",
      "Epoch: 3/20...  Training Step: 548...  Training loss: 1.9729...  8.2939 sec/batch\n",
      "Epoch: 3/20...  Training Step: 549...  Training loss: 1.9527...  8.3549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 1.9529...  8.2659 sec/batch\n",
      "Epoch: 3/20...  Training Step: 551...  Training loss: 1.9314...  8.2048 sec/batch\n",
      "Epoch: 3/20...  Training Step: 552...  Training loss: 1.9361...  9.9655 sec/batch\n",
      "Epoch: 3/20...  Training Step: 553...  Training loss: 1.9392...  10.9837 sec/batch\n",
      "Epoch: 3/20...  Training Step: 554...  Training loss: 1.9267...  9.0624 sec/batch\n",
      "Epoch: 3/20...  Training Step: 555...  Training loss: 1.8994...  8.4440 sec/batch\n",
      "Epoch: 3/20...  Training Step: 556...  Training loss: 1.9642...  8.2654 sec/batch\n",
      "Epoch: 3/20...  Training Step: 557...  Training loss: 1.9569...  8.2018 sec/batch\n",
      "Epoch: 3/20...  Training Step: 558...  Training loss: 1.9284...  9.9992 sec/batch\n",
      "Epoch: 3/20...  Training Step: 559...  Training loss: 1.9387...  8.2128 sec/batch\n",
      "Epoch: 3/20...  Training Step: 560...  Training loss: 1.9424...  8.1903 sec/batch\n",
      "Epoch: 3/20...  Training Step: 561...  Training loss: 1.9348...  8.3109 sec/batch\n",
      "Epoch: 3/20...  Training Step: 562...  Training loss: 1.9263...  8.3219 sec/batch\n",
      "Epoch: 3/20...  Training Step: 563...  Training loss: 1.9500...  8.4455 sec/batch\n",
      "Epoch: 3/20...  Training Step: 564...  Training loss: 1.9782...  8.1888 sec/batch\n",
      "Epoch: 3/20...  Training Step: 565...  Training loss: 1.9182...  8.2128 sec/batch\n",
      "Epoch: 3/20...  Training Step: 566...  Training loss: 1.9223...  8.2328 sec/batch\n",
      "Epoch: 3/20...  Training Step: 567...  Training loss: 1.9042...  8.3504 sec/batch\n",
      "Epoch: 3/20...  Training Step: 568...  Training loss: 1.9068...  8.3149 sec/batch\n",
      "Epoch: 3/20...  Training Step: 569...  Training loss: 1.9523...  8.3779 sec/batch\n",
      "Epoch: 3/20...  Training Step: 570...  Training loss: 1.9301...  8.4020 sec/batch\n",
      "Epoch: 3/20...  Training Step: 571...  Training loss: 1.9324...  8.3234 sec/batch\n",
      "Epoch: 3/20...  Training Step: 572...  Training loss: 1.9207...  8.4430 sec/batch\n",
      "Epoch: 3/20...  Training Step: 573...  Training loss: 1.9084...  8.9183 sec/batch\n",
      "Epoch: 3/20...  Training Step: 574...  Training loss: 1.9305...  9.1650 sec/batch\n",
      "Epoch: 3/20...  Training Step: 575...  Training loss: 1.9018...  9.7549 sec/batch\n",
      "Epoch: 3/20...  Training Step: 576...  Training loss: 1.8865...  8.7872 sec/batch\n",
      "Epoch: 3/20...  Training Step: 577...  Training loss: 1.8973...  8.3334 sec/batch\n",
      "Epoch: 3/20...  Training Step: 578...  Training loss: 1.9238...  8.2409 sec/batch\n",
      "Epoch: 3/20...  Training Step: 579...  Training loss: 1.9160...  8.3719 sec/batch\n",
      "Epoch: 3/20...  Training Step: 580...  Training loss: 1.9280...  8.3389 sec/batch\n",
      "Epoch: 3/20...  Training Step: 581...  Training loss: 1.9207...  8.2884 sec/batch\n",
      "Epoch: 3/20...  Training Step: 582...  Training loss: 1.9028...  8.2829 sec/batch\n",
      "Epoch: 3/20...  Training Step: 583...  Training loss: 1.9192...  8.4960 sec/batch\n",
      "Epoch: 3/20...  Training Step: 584...  Training loss: 1.9038...  9.4927 sec/batch\n",
      "Epoch: 3/20...  Training Step: 585...  Training loss: 1.8946...  8.7412 sec/batch\n",
      "Epoch: 3/20...  Training Step: 586...  Training loss: 1.9069...  8.8993 sec/batch\n",
      "Epoch: 3/20...  Training Step: 587...  Training loss: 1.9219...  8.2439 sec/batch\n",
      "Epoch: 3/20...  Training Step: 588...  Training loss: 1.8759...  8.2589 sec/batch\n",
      "Epoch: 3/20...  Training Step: 589...  Training loss: 1.9134...  8.2208 sec/batch\n",
      "Epoch: 3/20...  Training Step: 590...  Training loss: 1.8836...  8.2058 sec/batch\n",
      "Epoch: 3/20...  Training Step: 591...  Training loss: 1.8786...  8.2794 sec/batch\n",
      "Epoch: 3/20...  Training Step: 592...  Training loss: 1.9097...  8.2989 sec/batch\n",
      "Epoch: 3/20...  Training Step: 593...  Training loss: 1.8996...  8.1998 sec/batch\n",
      "Epoch: 3/20...  Training Step: 594...  Training loss: 1.8896...  8.1798 sec/batch\n",
      "Epoch: 4/20...  Training Step: 595...  Training loss: 1.9840...  8.2219 sec/batch\n",
      "Epoch: 4/20...  Training Step: 596...  Training loss: 1.8833...  8.4370 sec/batch\n",
      "Epoch: 4/20...  Training Step: 597...  Training loss: 1.8789...  8.2369 sec/batch\n",
      "Epoch: 4/20...  Training Step: 598...  Training loss: 1.8912...  8.4990 sec/batch\n",
      "Epoch: 4/20...  Training Step: 599...  Training loss: 1.8949...  8.8118 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.8552...  8.5000 sec/batch\n",
      "Epoch: 4/20...  Training Step: 601...  Training loss: 1.8936...  10.2633 sec/batch\n",
      "Epoch: 4/20...  Training Step: 602...  Training loss: 1.8738...  12.2897 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 603...  Training loss: 1.9232...  9.8180 sec/batch\n",
      "Epoch: 4/20...  Training Step: 604...  Training loss: 1.8932...  12.3903 sec/batch\n",
      "Epoch: 4/20...  Training Step: 605...  Training loss: 1.8743...  9.9120 sec/batch\n",
      "Epoch: 4/20...  Training Step: 606...  Training loss: 1.8727...  11.7634 sec/batch\n",
      "Epoch: 4/20...  Training Step: 607...  Training loss: 1.8880...  10.0762 sec/batch\n",
      "Epoch: 4/20...  Training Step: 608...  Training loss: 1.9206...  11.7273 sec/batch\n",
      "Epoch: 4/20...  Training Step: 609...  Training loss: 1.8853...  9.6639 sec/batch\n",
      "Epoch: 4/20...  Training Step: 610...  Training loss: 1.8661...  12.2517 sec/batch\n",
      "Epoch: 4/20...  Training Step: 611...  Training loss: 1.8872...  11.5022 sec/batch\n",
      "Epoch: 4/20...  Training Step: 612...  Training loss: 1.9172...  10.1112 sec/batch\n",
      "Epoch: 4/20...  Training Step: 613...  Training loss: 1.8817...  12.4398 sec/batch\n",
      "Epoch: 4/20...  Training Step: 614...  Training loss: 1.8925...  9.7940 sec/batch\n",
      "Epoch: 4/20...  Training Step: 615...  Training loss: 1.8766...  11.9980 sec/batch\n",
      "Epoch: 4/20...  Training Step: 616...  Training loss: 1.9107...  9.9150 sec/batch\n",
      "Epoch: 4/20...  Training Step: 617...  Training loss: 1.8825...  11.7784 sec/batch\n",
      "Epoch: 4/20...  Training Step: 618...  Training loss: 1.8857...  9.9656 sec/batch\n",
      "Epoch: 4/20...  Training Step: 619...  Training loss: 1.8776...  12.3327 sec/batch\n",
      "Epoch: 4/20...  Training Step: 620...  Training loss: 1.8496...  9.7910 sec/batch\n",
      "Epoch: 4/20...  Training Step: 621...  Training loss: 1.8586...  12.3778 sec/batch\n",
      "Epoch: 4/20...  Training Step: 622...  Training loss: 1.8829...  9.6519 sec/batch\n",
      "Epoch: 4/20...  Training Step: 623...  Training loss: 1.9100...  12.3883 sec/batch\n",
      "Epoch: 4/20...  Training Step: 624...  Training loss: 1.8897...  9.4117 sec/batch\n",
      "Epoch: 4/20...  Training Step: 625...  Training loss: 1.8769...  11.5142 sec/batch\n",
      "Epoch: 4/20...  Training Step: 626...  Training loss: 1.8469...  10.2458 sec/batch\n",
      "Epoch: 4/20...  Training Step: 627...  Training loss: 1.8740...  10.1642 sec/batch\n",
      "Epoch: 4/20...  Training Step: 628...  Training loss: 1.8991...  11.3170 sec/batch\n",
      "Epoch: 4/20...  Training Step: 629...  Training loss: 1.8640...  9.5513 sec/batch\n",
      "Epoch: 4/20...  Training Step: 630...  Training loss: 1.8810...  11.9785 sec/batch\n",
      "Epoch: 4/20...  Training Step: 631...  Training loss: 1.8699...  11.9165 sec/batch\n",
      "Epoch: 4/20...  Training Step: 632...  Training loss: 1.8406...  10.0912 sec/batch\n",
      "Epoch: 4/20...  Training Step: 633...  Training loss: 1.8356...  9.9411 sec/batch\n",
      "Epoch: 4/20...  Training Step: 634...  Training loss: 1.8419...  11.9850 sec/batch\n",
      "Epoch: 4/20...  Training Step: 635...  Training loss: 1.8491...  9.5878 sec/batch\n",
      "Epoch: 4/20...  Training Step: 636...  Training loss: 1.8760...  9.9160 sec/batch\n",
      "Epoch: 4/20...  Training Step: 637...  Training loss: 1.8413...  12.0816 sec/batch\n",
      "Epoch: 4/20...  Training Step: 638...  Training loss: 1.8374...  9.7839 sec/batch\n",
      "Epoch: 4/20...  Training Step: 639...  Training loss: 1.8750...  11.6473 sec/batch\n",
      "Epoch: 4/20...  Training Step: 640...  Training loss: 1.8237...  9.8695 sec/batch\n",
      "Epoch: 4/20...  Training Step: 641...  Training loss: 1.8681...  10.4874 sec/batch\n",
      "Epoch: 4/20...  Training Step: 642...  Training loss: 1.8478...  11.2910 sec/batch\n",
      "Epoch: 4/20...  Training Step: 643...  Training loss: 1.8399...  12.4043 sec/batch\n",
      "Epoch: 4/20...  Training Step: 644...  Training loss: 1.8966...  9.6509 sec/batch\n",
      "Epoch: 4/20...  Training Step: 645...  Training loss: 1.8287...  11.9405 sec/batch\n",
      "Epoch: 4/20...  Training Step: 646...  Training loss: 1.9139...  9.7849 sec/batch\n",
      "Epoch: 4/20...  Training Step: 647...  Training loss: 1.8432...  11.5932 sec/batch\n",
      "Epoch: 4/20...  Training Step: 648...  Training loss: 1.8501...  9.7504 sec/batch\n",
      "Epoch: 4/20...  Training Step: 649...  Training loss: 1.8428...  9.9351 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.8674...  9.9731 sec/batch\n",
      "Epoch: 4/20...  Training Step: 651...  Training loss: 1.8614...  9.9591 sec/batch\n",
      "Epoch: 4/20...  Training Step: 652...  Training loss: 1.8525...  9.5468 sec/batch\n",
      "Epoch: 4/20...  Training Step: 653...  Training loss: 1.8282...  12.0606 sec/batch\n",
      "Epoch: 4/20...  Training Step: 654...  Training loss: 1.8910...  10.0612 sec/batch\n",
      "Epoch: 4/20...  Training Step: 655...  Training loss: 1.8440...  12.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 656...  Training loss: 1.8860...  9.8340 sec/batch\n",
      "Epoch: 4/20...  Training Step: 657...  Training loss: 1.8884...  11.5747 sec/batch\n",
      "Epoch: 4/20...  Training Step: 658...  Training loss: 1.8646...  9.9200 sec/batch\n",
      "Epoch: 4/20...  Training Step: 659...  Training loss: 1.8338...  12.0886 sec/batch\n",
      "Epoch: 4/20...  Training Step: 660...  Training loss: 1.8906...  9.5123 sec/batch\n",
      "Epoch: 4/20...  Training Step: 661...  Training loss: 1.8659...  12.5549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 662...  Training loss: 1.8241...  9.8920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 663...  Training loss: 1.8362...  11.8703 sec/batch\n",
      "Epoch: 4/20...  Training Step: 664...  Training loss: 1.8380...  8.9694 sec/batch\n",
      "Epoch: 4/20...  Training Step: 665...  Training loss: 1.8763...  9.2786 sec/batch\n",
      "Epoch: 4/20...  Training Step: 666...  Training loss: 1.8638...  9.8920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 667...  Training loss: 1.8739...  9.2626 sec/batch\n",
      "Epoch: 4/20...  Training Step: 668...  Training loss: 1.8313...  9.0039 sec/batch\n",
      "Epoch: 4/20...  Training Step: 669...  Training loss: 1.8444...  8.4605 sec/batch\n",
      "Epoch: 4/20...  Training Step: 670...  Training loss: 1.8735...  9.1565 sec/batch\n",
      "Epoch: 4/20...  Training Step: 671...  Training loss: 1.8507...  9.0124 sec/batch\n",
      "Epoch: 4/20...  Training Step: 672...  Training loss: 1.8398...  8.8628 sec/batch\n",
      "Epoch: 4/20...  Training Step: 673...  Training loss: 1.8084...  8.7812 sec/batch\n",
      "Epoch: 4/20...  Training Step: 674...  Training loss: 1.8277...  11.8148 sec/batch\n",
      "Epoch: 4/20...  Training Step: 675...  Training loss: 1.8007...  8.3905 sec/batch\n",
      "Epoch: 4/20...  Training Step: 676...  Training loss: 1.8535...  9.2301 sec/batch\n",
      "Epoch: 4/20...  Training Step: 677...  Training loss: 1.7983...  8.6011 sec/batch\n",
      "Epoch: 4/20...  Training Step: 678...  Training loss: 1.8380...  9.0650 sec/batch\n",
      "Epoch: 4/20...  Training Step: 679...  Training loss: 1.7930...  8.7762 sec/batch\n",
      "Epoch: 4/20...  Training Step: 680...  Training loss: 1.8073...  9.0464 sec/batch\n",
      "Epoch: 4/20...  Training Step: 681...  Training loss: 1.8173...  8.7012 sec/batch\n",
      "Epoch: 4/20...  Training Step: 682...  Training loss: 1.8078...  9.5045 sec/batch\n",
      "Epoch: 4/20...  Training Step: 683...  Training loss: 1.8029...  11.2030 sec/batch\n",
      "Epoch: 4/20...  Training Step: 684...  Training loss: 1.8367...  10.4914 sec/batch\n",
      "Epoch: 4/20...  Training Step: 685...  Training loss: 1.7998...  10.2978 sec/batch\n",
      "Epoch: 4/20...  Training Step: 686...  Training loss: 1.8091...  9.7469 sec/batch\n",
      "Epoch: 4/20...  Training Step: 687...  Training loss: 1.7947...  9.6729 sec/batch\n",
      "Epoch: 4/20...  Training Step: 688...  Training loss: 1.8050...  10.2538 sec/batch\n",
      "Epoch: 4/20...  Training Step: 689...  Training loss: 1.8031...  10.9097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 690...  Training loss: 1.8179...  9.1535 sec/batch\n",
      "Epoch: 4/20...  Training Step: 691...  Training loss: 1.8148...  9.7204 sec/batch\n",
      "Epoch: 4/20...  Training Step: 692...  Training loss: 1.7834...  10.6366 sec/batch\n",
      "Epoch: 4/20...  Training Step: 693...  Training loss: 1.7991...  9.7209 sec/batch\n",
      "Epoch: 4/20...  Training Step: 694...  Training loss: 1.7852...  9.4072 sec/batch\n",
      "Epoch: 4/20...  Training Step: 695...  Training loss: 1.8219...  8.7232 sec/batch\n",
      "Epoch: 4/20...  Training Step: 696...  Training loss: 1.8090...  9.6368 sec/batch\n",
      "Epoch: 4/20...  Training Step: 697...  Training loss: 1.8055...  9.4567 sec/batch\n",
      "Epoch: 4/20...  Training Step: 698...  Training loss: 1.7964...  8.6141 sec/batch\n",
      "Epoch: 4/20...  Training Step: 699...  Training loss: 1.8007...  9.4247 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.8091...  8.9329 sec/batch\n",
      "Epoch: 4/20...  Training Step: 701...  Training loss: 1.8132...  9.7759 sec/batch\n",
      "Epoch: 4/20...  Training Step: 702...  Training loss: 1.8143...  9.3136 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 703...  Training loss: 1.8249...  9.6914 sec/batch\n",
      "Epoch: 4/20...  Training Step: 704...  Training loss: 1.8206...  9.6458 sec/batch\n",
      "Epoch: 4/20...  Training Step: 705...  Training loss: 1.8024...  9.4837 sec/batch\n",
      "Epoch: 4/20...  Training Step: 706...  Training loss: 1.7943...  9.3887 sec/batch\n",
      "Epoch: 4/20...  Training Step: 707...  Training loss: 1.8073...  9.5808 sec/batch\n",
      "Epoch: 4/20...  Training Step: 708...  Training loss: 1.7994...  9.8270 sec/batch\n",
      "Epoch: 4/20...  Training Step: 709...  Training loss: 1.7887...  9.6819 sec/batch\n",
      "Epoch: 4/20...  Training Step: 710...  Training loss: 1.7755...  10.0617 sec/batch\n",
      "Epoch: 4/20...  Training Step: 711...  Training loss: 1.8178...  9.0344 sec/batch\n",
      "Epoch: 4/20...  Training Step: 712...  Training loss: 1.7921...  9.4027 sec/batch\n",
      "Epoch: 4/20...  Training Step: 713...  Training loss: 1.8050...  9.8080 sec/batch\n",
      "Epoch: 4/20...  Training Step: 714...  Training loss: 1.8032...  9.4377 sec/batch\n",
      "Epoch: 4/20...  Training Step: 715...  Training loss: 1.8098...  8.8183 sec/batch\n",
      "Epoch: 4/20...  Training Step: 716...  Training loss: 1.7787...  9.0875 sec/batch\n",
      "Epoch: 4/20...  Training Step: 717...  Training loss: 1.7723...  9.8030 sec/batch\n",
      "Epoch: 4/20...  Training Step: 718...  Training loss: 1.8213...  10.5165 sec/batch\n",
      "Epoch: 4/20...  Training Step: 719...  Training loss: 1.7941...  10.5955 sec/batch\n",
      "Epoch: 4/20...  Training Step: 720...  Training loss: 1.7530...  10.5155 sec/batch\n",
      "Epoch: 4/20...  Training Step: 721...  Training loss: 1.8162...  9.4327 sec/batch\n",
      "Epoch: 4/20...  Training Step: 722...  Training loss: 1.8166...  9.1630 sec/batch\n",
      "Epoch: 4/20...  Training Step: 723...  Training loss: 1.7918...  8.9454 sec/batch\n",
      "Epoch: 4/20...  Training Step: 724...  Training loss: 1.7933...  8.6682 sec/batch\n",
      "Epoch: 4/20...  Training Step: 725...  Training loss: 1.7665...  8.5916 sec/batch\n",
      "Epoch: 4/20...  Training Step: 726...  Training loss: 1.7729...  8.7622 sec/batch\n",
      "Epoch: 4/20...  Training Step: 727...  Training loss: 1.7990...  9.4047 sec/batch\n",
      "Epoch: 4/20...  Training Step: 728...  Training loss: 1.7985...  9.6994 sec/batch\n",
      "Epoch: 4/20...  Training Step: 729...  Training loss: 1.8071...  8.1758 sec/batch\n",
      "Epoch: 4/20...  Training Step: 730...  Training loss: 1.7937...  8.7532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 731...  Training loss: 1.8086...  9.0935 sec/batch\n",
      "Epoch: 4/20...  Training Step: 732...  Training loss: 1.8036...  8.3454 sec/batch\n",
      "Epoch: 4/20...  Training Step: 733...  Training loss: 1.8197...  8.6411 sec/batch\n",
      "Epoch: 4/20...  Training Step: 734...  Training loss: 1.7870...  9.1625 sec/batch\n",
      "Epoch: 4/20...  Training Step: 735...  Training loss: 1.8335...  9.0354 sec/batch\n",
      "Epoch: 4/20...  Training Step: 736...  Training loss: 1.7847...  9.4757 sec/batch\n",
      "Epoch: 4/20...  Training Step: 737...  Training loss: 1.7937...  8.8693 sec/batch\n",
      "Epoch: 4/20...  Training Step: 738...  Training loss: 1.7995...  8.4910 sec/batch\n",
      "Epoch: 4/20...  Training Step: 739...  Training loss: 1.7751...  9.6469 sec/batch\n",
      "Epoch: 4/20...  Training Step: 740...  Training loss: 1.7972...  9.1975 sec/batch\n",
      "Epoch: 4/20...  Training Step: 741...  Training loss: 1.7917...  11.3331 sec/batch\n",
      "Epoch: 4/20...  Training Step: 742...  Training loss: 1.8219...  12.3393 sec/batch\n",
      "Epoch: 4/20...  Training Step: 743...  Training loss: 1.7972...  8.9664 sec/batch\n",
      "Epoch: 4/20...  Training Step: 744...  Training loss: 1.7769...  8.5601 sec/batch\n",
      "Epoch: 4/20...  Training Step: 745...  Training loss: 1.7712...  9.4157 sec/batch\n",
      "Epoch: 4/20...  Training Step: 746...  Training loss: 1.8101...  8.4290 sec/batch\n",
      "Epoch: 4/20...  Training Step: 747...  Training loss: 1.7915...  8.7752 sec/batch\n",
      "Epoch: 4/20...  Training Step: 748...  Training loss: 1.7911...  8.9068 sec/batch\n",
      "Epoch: 4/20...  Training Step: 749...  Training loss: 1.7871...  8.9724 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.7924...  9.0895 sec/batch\n",
      "Epoch: 4/20...  Training Step: 751...  Training loss: 1.7954...  8.1438 sec/batch\n",
      "Epoch: 4/20...  Training Step: 752...  Training loss: 1.7871...  9.3396 sec/batch\n",
      "Epoch: 4/20...  Training Step: 753...  Training loss: 1.7466...  8.8723 sec/batch\n",
      "Epoch: 4/20...  Training Step: 754...  Training loss: 1.8055...  10.8087 sec/batch\n",
      "Epoch: 4/20...  Training Step: 755...  Training loss: 1.8120...  12.0185 sec/batch\n",
      "Epoch: 4/20...  Training Step: 756...  Training loss: 1.7849...  10.1457 sec/batch\n",
      "Epoch: 4/20...  Training Step: 757...  Training loss: 1.7941...  8.8693 sec/batch\n",
      "Epoch: 4/20...  Training Step: 758...  Training loss: 1.7826...  10.4324 sec/batch\n",
      "Epoch: 4/20...  Training Step: 759...  Training loss: 1.7761...  9.6709 sec/batch\n",
      "Epoch: 4/20...  Training Step: 760...  Training loss: 1.7691...  9.0364 sec/batch\n",
      "Epoch: 4/20...  Training Step: 761...  Training loss: 1.7932...  10.1067 sec/batch\n",
      "Epoch: 4/20...  Training Step: 762...  Training loss: 1.8352...  10.0962 sec/batch\n",
      "Epoch: 4/20...  Training Step: 763...  Training loss: 1.7692...  10.9698 sec/batch\n",
      "Epoch: 4/20...  Training Step: 764...  Training loss: 1.7758...  8.8233 sec/batch\n",
      "Epoch: 4/20...  Training Step: 765...  Training loss: 1.7654...  8.8163 sec/batch\n",
      "Epoch: 4/20...  Training Step: 766...  Training loss: 1.7589...  10.8347 sec/batch\n",
      "Epoch: 4/20...  Training Step: 767...  Training loss: 1.7993...  10.6621 sec/batch\n",
      "Epoch: 4/20...  Training Step: 768...  Training loss: 1.7822...  9.4697 sec/batch\n",
      "Epoch: 4/20...  Training Step: 769...  Training loss: 1.7889...  9.4977 sec/batch\n",
      "Epoch: 4/20...  Training Step: 770...  Training loss: 1.7651...  9.7479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 771...  Training loss: 1.7545...  9.0624 sec/batch\n",
      "Epoch: 4/20...  Training Step: 772...  Training loss: 1.7807...  9.3306 sec/batch\n",
      "Epoch: 4/20...  Training Step: 773...  Training loss: 1.7423...  9.6754 sec/batch\n",
      "Epoch: 4/20...  Training Step: 774...  Training loss: 1.7365...  9.4337 sec/batch\n",
      "Epoch: 4/20...  Training Step: 775...  Training loss: 1.7441...  8.5841 sec/batch\n",
      "Epoch: 4/20...  Training Step: 776...  Training loss: 1.7693...  9.0009 sec/batch\n",
      "Epoch: 4/20...  Training Step: 777...  Training loss: 1.7715...  9.1215 sec/batch\n",
      "Epoch: 4/20...  Training Step: 778...  Training loss: 1.7844...  9.8090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 779...  Training loss: 1.7642...  9.9276 sec/batch\n",
      "Epoch: 4/20...  Training Step: 780...  Training loss: 1.7514...  8.9334 sec/batch\n",
      "Epoch: 4/20...  Training Step: 781...  Training loss: 1.7817...  9.6228 sec/batch\n",
      "Epoch: 4/20...  Training Step: 782...  Training loss: 1.7476...  9.8435 sec/batch\n",
      "Epoch: 4/20...  Training Step: 783...  Training loss: 1.7551...  9.9886 sec/batch\n",
      "Epoch: 4/20...  Training Step: 784...  Training loss: 1.7622...  9.4147 sec/batch\n",
      "Epoch: 4/20...  Training Step: 785...  Training loss: 1.7631...  10.9228 sec/batch\n",
      "Epoch: 4/20...  Training Step: 786...  Training loss: 1.7333...  9.7204 sec/batch\n",
      "Epoch: 4/20...  Training Step: 787...  Training loss: 1.7628...  9.2235 sec/batch\n",
      "Epoch: 4/20...  Training Step: 788...  Training loss: 1.7374...  9.4067 sec/batch\n",
      "Epoch: 4/20...  Training Step: 789...  Training loss: 1.7241...  8.5501 sec/batch\n",
      "Epoch: 4/20...  Training Step: 790...  Training loss: 1.7584...  8.4820 sec/batch\n",
      "Epoch: 4/20...  Training Step: 791...  Training loss: 1.7554...  9.8710 sec/batch\n",
      "Epoch: 4/20...  Training Step: 792...  Training loss: 1.7415...  10.6671 sec/batch\n",
      "Epoch: 5/20...  Training Step: 793...  Training loss: 1.8490...  9.9991 sec/batch\n",
      "Epoch: 5/20...  Training Step: 794...  Training loss: 1.7477...  9.3426 sec/batch\n",
      "Epoch: 5/20...  Training Step: 795...  Training loss: 1.7413...  9.6699 sec/batch\n",
      "Epoch: 5/20...  Training Step: 796...  Training loss: 1.7553...  9.5068 sec/batch\n",
      "Epoch: 5/20...  Training Step: 797...  Training loss: 1.7403...  9.4287 sec/batch\n",
      "Epoch: 5/20...  Training Step: 798...  Training loss: 1.7088...  9.4958 sec/batch\n",
      "Epoch: 5/20...  Training Step: 799...  Training loss: 1.7518...  9.1645 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.7301...  9.3887 sec/batch\n",
      "Epoch: 5/20...  Training Step: 801...  Training loss: 1.7771...  11.4477 sec/batch\n",
      "Epoch: 5/20...  Training Step: 802...  Training loss: 1.7462...  9.3937 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 803...  Training loss: 1.7274...  10.8697 sec/batch\n",
      "Epoch: 5/20...  Training Step: 804...  Training loss: 1.7239...  9.9642 sec/batch\n",
      "Epoch: 5/20...  Training Step: 805...  Training loss: 1.7423...  8.8333 sec/batch\n",
      "Epoch: 5/20...  Training Step: 806...  Training loss: 1.7830...  9.6188 sec/batch\n",
      "Epoch: 5/20...  Training Step: 807...  Training loss: 1.7336...  9.5493 sec/batch\n",
      "Epoch: 5/20...  Training Step: 808...  Training loss: 1.7234...  10.0381 sec/batch\n",
      "Epoch: 5/20...  Training Step: 809...  Training loss: 1.7508...  9.4817 sec/batch\n",
      "Epoch: 5/20...  Training Step: 810...  Training loss: 1.7885...  10.7496 sec/batch\n",
      "Epoch: 5/20...  Training Step: 811...  Training loss: 1.7506...  10.0371 sec/batch\n",
      "Epoch: 5/20...  Training Step: 812...  Training loss: 1.7622...  10.7746 sec/batch\n",
      "Epoch: 5/20...  Training Step: 813...  Training loss: 1.7429...  10.3784 sec/batch\n",
      "Epoch: 5/20...  Training Step: 814...  Training loss: 1.7825...  8.4900 sec/batch\n",
      "Epoch: 5/20...  Training Step: 815...  Training loss: 1.7412...  8.7812 sec/batch\n",
      "Epoch: 5/20...  Training Step: 816...  Training loss: 1.7491...  8.1208 sec/batch\n",
      "Epoch: 5/20...  Training Step: 817...  Training loss: 1.7608...  8.9393 sec/batch\n",
      "Epoch: 5/20...  Training Step: 818...  Training loss: 1.7077...  9.4597 sec/batch\n",
      "Epoch: 5/20...  Training Step: 819...  Training loss: 1.7104...  9.1796 sec/batch\n",
      "Epoch: 5/20...  Training Step: 820...  Training loss: 1.7574...  8.7362 sec/batch\n",
      "Epoch: 5/20...  Training Step: 821...  Training loss: 1.7680...  8.7272 sec/batch\n",
      "Epoch: 5/20...  Training Step: 822...  Training loss: 1.7584...  9.8950 sec/batch\n",
      "Epoch: 5/20...  Training Step: 823...  Training loss: 1.7404...  8.9829 sec/batch\n",
      "Epoch: 5/20...  Training Step: 824...  Training loss: 1.7151...  9.5998 sec/batch\n",
      "Epoch: 5/20...  Training Step: 825...  Training loss: 1.7515...  8.5571 sec/batch\n",
      "Epoch: 5/20...  Training Step: 826...  Training loss: 1.7594...  9.1170 sec/batch\n",
      "Epoch: 5/20...  Training Step: 827...  Training loss: 1.7196...  9.2105 sec/batch\n",
      "Epoch: 5/20...  Training Step: 828...  Training loss: 1.7373...  9.2015 sec/batch\n",
      "Epoch: 5/20...  Training Step: 829...  Training loss: 1.7208...  8.7267 sec/batch\n",
      "Epoch: 5/20...  Training Step: 830...  Training loss: 1.7015...  8.7642 sec/batch\n",
      "Epoch: 5/20...  Training Step: 831...  Training loss: 1.6955...  9.0865 sec/batch\n",
      "Epoch: 5/20...  Training Step: 832...  Training loss: 1.7045...  8.5671 sec/batch\n",
      "Epoch: 5/20...  Training Step: 833...  Training loss: 1.7124...  8.9574 sec/batch\n",
      "Epoch: 5/20...  Training Step: 834...  Training loss: 1.7418...  9.3847 sec/batch\n",
      "Epoch: 5/20...  Training Step: 835...  Training loss: 1.7067...  9.2696 sec/batch\n",
      "Epoch: 5/20...  Training Step: 836...  Training loss: 1.7015...  8.8813 sec/batch\n",
      "Epoch: 5/20...  Training Step: 837...  Training loss: 1.7461...  8.9113 sec/batch\n",
      "Epoch: 5/20...  Training Step: 838...  Training loss: 1.6872...  9.1985 sec/batch\n",
      "Epoch: 5/20...  Training Step: 839...  Training loss: 1.7257...  9.3231 sec/batch\n",
      "Epoch: 5/20...  Training Step: 840...  Training loss: 1.7086...  8.6281 sec/batch\n",
      "Epoch: 5/20...  Training Step: 841...  Training loss: 1.7045...  8.6481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 842...  Training loss: 1.7625...  10.8987 sec/batch\n",
      "Epoch: 5/20...  Training Step: 843...  Training loss: 1.7019...  9.7014 sec/batch\n",
      "Epoch: 5/20...  Training Step: 844...  Training loss: 1.7935...  8.7392 sec/batch\n",
      "Epoch: 5/20...  Training Step: 845...  Training loss: 1.7152...  9.2225 sec/batch\n",
      "Epoch: 5/20...  Training Step: 846...  Training loss: 1.7360...  8.9529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 847...  Training loss: 1.7135...  9.3196 sec/batch\n",
      "Epoch: 5/20...  Training Step: 848...  Training loss: 1.7413...  9.7359 sec/batch\n",
      "Epoch: 5/20...  Training Step: 849...  Training loss: 1.7546...  9.8525 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.7214...  9.3396 sec/batch\n",
      "Epoch: 5/20...  Training Step: 851...  Training loss: 1.7098...  8.0477 sec/batch\n",
      "Epoch: 5/20...  Training Step: 852...  Training loss: 1.7653...  8.1063 sec/batch\n",
      "Epoch: 5/20...  Training Step: 853...  Training loss: 1.7233...  9.6178 sec/batch\n",
      "Epoch: 5/20...  Training Step: 854...  Training loss: 1.7838...  9.0514 sec/batch\n",
      "Epoch: 5/20...  Training Step: 855...  Training loss: 1.7505...  9.7484 sec/batch\n",
      "Epoch: 5/20...  Training Step: 856...  Training loss: 1.7533...  10.1017 sec/batch\n",
      "Epoch: 5/20...  Training Step: 857...  Training loss: 1.7195...  10.6586 sec/batch\n",
      "Epoch: 5/20...  Training Step: 858...  Training loss: 1.7477...  10.7696 sec/batch\n",
      "Epoch: 5/20...  Training Step: 859...  Training loss: 1.7389...  9.2351 sec/batch\n",
      "Epoch: 5/20...  Training Step: 860...  Training loss: 1.6862...  11.1609 sec/batch\n",
      "Epoch: 5/20...  Training Step: 861...  Training loss: 1.7193...  10.4834 sec/batch\n",
      "Epoch: 5/20...  Training Step: 862...  Training loss: 1.7109...  8.7292 sec/batch\n",
      "Epoch: 5/20...  Training Step: 863...  Training loss: 1.7595...  8.6291 sec/batch\n",
      "Epoch: 5/20...  Training Step: 864...  Training loss: 1.7405...  9.4457 sec/batch\n",
      "Epoch: 5/20...  Training Step: 865...  Training loss: 1.7469...  9.2266 sec/batch\n",
      "Epoch: 5/20...  Training Step: 866...  Training loss: 1.7031...  9.2055 sec/batch\n",
      "Epoch: 5/20...  Training Step: 867...  Training loss: 1.7114...  8.4580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 868...  Training loss: 1.7550...  8.7167 sec/batch\n",
      "Epoch: 5/20...  Training Step: 869...  Training loss: 1.7241...  8.8983 sec/batch\n",
      "Epoch: 5/20...  Training Step: 870...  Training loss: 1.7124...  9.1135 sec/batch\n",
      "Epoch: 5/20...  Training Step: 871...  Training loss: 1.6873...  8.7152 sec/batch\n",
      "Epoch: 5/20...  Training Step: 872...  Training loss: 1.7082...  9.1185 sec/batch\n",
      "Epoch: 5/20...  Training Step: 873...  Training loss: 1.6725...  9.3156 sec/batch\n",
      "Epoch: 5/20...  Training Step: 874...  Training loss: 1.7306...  11.7363 sec/batch\n",
      "Epoch: 5/20...  Training Step: 875...  Training loss: 1.6690...  9.7099 sec/batch\n",
      "Epoch: 5/20...  Training Step: 876...  Training loss: 1.7088...  9.7599 sec/batch\n",
      "Epoch: 5/20...  Training Step: 877...  Training loss: 1.6841...  9.3957 sec/batch\n",
      "Epoch: 5/20...  Training Step: 878...  Training loss: 1.6940...  9.5858 sec/batch\n",
      "Epoch: 5/20...  Training Step: 879...  Training loss: 1.6901...  11.2720 sec/batch\n",
      "Epoch: 5/20...  Training Step: 880...  Training loss: 1.6848...  10.1482 sec/batch\n",
      "Epoch: 5/20...  Training Step: 881...  Training loss: 1.6701...  10.7877 sec/batch\n",
      "Epoch: 5/20...  Training Step: 882...  Training loss: 1.7095...  10.3413 sec/batch\n",
      "Epoch: 5/20...  Training Step: 883...  Training loss: 1.6776...  10.2208 sec/batch\n",
      "Epoch: 5/20...  Training Step: 884...  Training loss: 1.6898...  11.9605 sec/batch\n",
      "Epoch: 5/20...  Training Step: 885...  Training loss: 1.6686...  9.5027 sec/batch\n",
      "Epoch: 5/20...  Training Step: 886...  Training loss: 1.6860...  8.9384 sec/batch\n",
      "Epoch: 5/20...  Training Step: 887...  Training loss: 1.6807...  8.7262 sec/batch\n",
      "Epoch: 5/20...  Training Step: 888...  Training loss: 1.6961...  10.9037 sec/batch\n",
      "Epoch: 5/20...  Training Step: 889...  Training loss: 1.6979...  12.0280 sec/batch\n",
      "Epoch: 5/20...  Training Step: 890...  Training loss: 1.6676...  10.2020 sec/batch\n",
      "Epoch: 5/20...  Training Step: 891...  Training loss: 1.6827...  9.0789 sec/batch\n",
      "Epoch: 5/20...  Training Step: 892...  Training loss: 1.6488...  9.7976 sec/batch\n",
      "Epoch: 5/20...  Training Step: 893...  Training loss: 1.7031...  9.2020 sec/batch\n",
      "Epoch: 5/20...  Training Step: 894...  Training loss: 1.6883...  10.3523 sec/batch\n",
      "Epoch: 5/20...  Training Step: 895...  Training loss: 1.6824...  9.1470 sec/batch\n",
      "Epoch: 5/20...  Training Step: 896...  Training loss: 1.6812...  9.0384 sec/batch\n",
      "Epoch: 5/20...  Training Step: 897...  Training loss: 1.6824...  9.8955 sec/batch\n",
      "Epoch: 5/20...  Training Step: 898...  Training loss: 1.6940...  9.2641 sec/batch\n",
      "Epoch: 5/20...  Training Step: 899...  Training loss: 1.6986...  9.4327 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.6908...  9.0414 sec/batch\n",
      "Epoch: 5/20...  Training Step: 901...  Training loss: 1.7064...  9.0104 sec/batch\n",
      "Epoch: 5/20...  Training Step: 902...  Training loss: 1.7133...  9.2896 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 903...  Training loss: 1.6843...  9.5868 sec/batch\n",
      "Epoch: 5/20...  Training Step: 904...  Training loss: 1.6865...  9.5908 sec/batch\n",
      "Epoch: 5/20...  Training Step: 905...  Training loss: 1.6796...  10.6511 sec/batch\n",
      "Epoch: 5/20...  Training Step: 906...  Training loss: 1.6818...  9.4627 sec/batch\n",
      "Epoch: 5/20...  Training Step: 907...  Training loss: 1.6703...  8.7762 sec/batch\n",
      "Epoch: 5/20...  Training Step: 908...  Training loss: 1.6444...  8.2469 sec/batch\n",
      "Epoch: 5/20...  Training Step: 909...  Training loss: 1.6942...  8.4850 sec/batch\n",
      "Epoch: 5/20...  Training Step: 910...  Training loss: 1.6789...  10.0131 sec/batch\n",
      "Epoch: 5/20...  Training Step: 911...  Training loss: 1.6910...  9.8685 sec/batch\n",
      "Epoch: 5/20...  Training Step: 912...  Training loss: 1.6868...  9.0324 sec/batch\n",
      "Epoch: 5/20...  Training Step: 913...  Training loss: 1.6895...  9.0915 sec/batch\n",
      "Epoch: 5/20...  Training Step: 914...  Training loss: 1.6488...  8.7742 sec/batch\n",
      "Epoch: 5/20...  Training Step: 915...  Training loss: 1.6537...  9.2616 sec/batch\n",
      "Epoch: 5/20...  Training Step: 916...  Training loss: 1.7040...  8.5561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 917...  Training loss: 1.6819...  8.6541 sec/batch\n",
      "Epoch: 5/20...  Training Step: 918...  Training loss: 1.6416...  9.6229 sec/batch\n",
      "Epoch: 5/20...  Training Step: 919...  Training loss: 1.6948...  9.8260 sec/batch\n",
      "Epoch: 5/20...  Training Step: 920...  Training loss: 1.6987...  8.1638 sec/batch\n",
      "Epoch: 5/20...  Training Step: 921...  Training loss: 1.6848...  8.1843 sec/batch\n",
      "Epoch: 5/20...  Training Step: 922...  Training loss: 1.6560...  8.0677 sec/batch\n",
      "Epoch: 5/20...  Training Step: 923...  Training loss: 1.6482...  8.4740 sec/batch\n",
      "Epoch: 5/20...  Training Step: 924...  Training loss: 1.6573...  8.7492 sec/batch\n",
      "Epoch: 5/20...  Training Step: 925...  Training loss: 1.6974...  8.6507 sec/batch\n",
      "Epoch: 5/20...  Training Step: 926...  Training loss: 1.6848...  8.0797 sec/batch\n",
      "Epoch: 5/20...  Training Step: 927...  Training loss: 1.6871...  9.0774 sec/batch\n",
      "Epoch: 5/20...  Training Step: 928...  Training loss: 1.6896...  8.0252 sec/batch\n",
      "Epoch: 5/20...  Training Step: 929...  Training loss: 1.7044...  8.3789 sec/batch\n",
      "Epoch: 5/20...  Training Step: 930...  Training loss: 1.6819...  8.9764 sec/batch\n",
      "Epoch: 5/20...  Training Step: 931...  Training loss: 1.7002...  8.0657 sec/batch\n",
      "Epoch: 5/20...  Training Step: 932...  Training loss: 1.6832...  8.2459 sec/batch\n",
      "Epoch: 5/20...  Training Step: 933...  Training loss: 1.7348...  9.1165 sec/batch\n",
      "Epoch: 5/20...  Training Step: 934...  Training loss: 1.6715...  8.7582 sec/batch\n",
      "Epoch: 5/20...  Training Step: 935...  Training loss: 1.6818...  8.7572 sec/batch\n",
      "Epoch: 5/20...  Training Step: 936...  Training loss: 1.6927...  9.1295 sec/batch\n",
      "Epoch: 5/20...  Training Step: 937...  Training loss: 1.6605...  9.2526 sec/batch\n",
      "Epoch: 5/20...  Training Step: 938...  Training loss: 1.7018...  8.6331 sec/batch\n",
      "Epoch: 5/20...  Training Step: 939...  Training loss: 1.6921...  9.0805 sec/batch\n",
      "Epoch: 5/20...  Training Step: 940...  Training loss: 1.7097...  9.0484 sec/batch\n",
      "Epoch: 5/20...  Training Step: 941...  Training loss: 1.6948...  9.4397 sec/batch\n",
      "Epoch: 5/20...  Training Step: 942...  Training loss: 1.6734...  10.2665 sec/batch\n",
      "Epoch: 5/20...  Training Step: 943...  Training loss: 1.6524...  9.5943 sec/batch\n",
      "Epoch: 5/20...  Training Step: 944...  Training loss: 1.6932...  9.0374 sec/batch\n",
      "Epoch: 5/20...  Training Step: 945...  Training loss: 1.6923...  11.1617 sec/batch\n",
      "Epoch: 5/20...  Training Step: 946...  Training loss: 1.6901...  9.6854 sec/batch\n",
      "Epoch: 5/20...  Training Step: 947...  Training loss: 1.6804...  9.4177 sec/batch\n",
      "Epoch: 5/20...  Training Step: 948...  Training loss: 1.6775...  9.9946 sec/batch\n",
      "Epoch: 5/20...  Training Step: 949...  Training loss: 1.6895...  12.1686 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.6711...  11.1909 sec/batch\n",
      "Epoch: 5/20...  Training Step: 951...  Training loss: 1.6367...  10.9868 sec/batch\n",
      "Epoch: 5/20...  Training Step: 952...  Training loss: 1.6987...  11.7788 sec/batch\n",
      "Epoch: 5/20...  Training Step: 953...  Training loss: 1.7017...  10.0056 sec/batch\n",
      "Epoch: 5/20...  Training Step: 954...  Training loss: 1.6752...  10.5455 sec/batch\n",
      "Epoch: 5/20...  Training Step: 955...  Training loss: 1.6900...  10.3023 sec/batch\n",
      "Epoch: 5/20...  Training Step: 956...  Training loss: 1.6787...  9.7785 sec/batch\n",
      "Epoch: 5/20...  Training Step: 957...  Training loss: 1.6686...  10.9228 sec/batch\n",
      "Epoch: 5/20...  Training Step: 958...  Training loss: 1.6700...  11.5696 sec/batch\n",
      "Epoch: 5/20...  Training Step: 959...  Training loss: 1.7001...  9.5343 sec/batch\n",
      "Epoch: 5/20...  Training Step: 960...  Training loss: 1.7436...  10.8340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 961...  Training loss: 1.6612...  10.8638 sec/batch\n",
      "Epoch: 5/20...  Training Step: 962...  Training loss: 1.6702...  12.2846 sec/batch\n",
      "Epoch: 5/20...  Training Step: 963...  Training loss: 1.6639...  12.3443 sec/batch\n",
      "Epoch: 5/20...  Training Step: 964...  Training loss: 1.6587...  10.7556 sec/batch\n",
      "Epoch: 5/20...  Training Step: 965...  Training loss: 1.6922...  10.9718 sec/batch\n",
      "Epoch: 5/20...  Training Step: 966...  Training loss: 1.6798...  10.3463 sec/batch\n",
      "Epoch: 5/20...  Training Step: 967...  Training loss: 1.6879...  10.0942 sec/batch\n",
      "Epoch: 5/20...  Training Step: 968...  Training loss: 1.6570...  8.7332 sec/batch\n",
      "Epoch: 5/20...  Training Step: 969...  Training loss: 1.6572...  9.3536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 970...  Training loss: 1.6900...  9.5228 sec/batch\n",
      "Epoch: 5/20...  Training Step: 971...  Training loss: 1.6542...  8.9053 sec/batch\n",
      "Epoch: 5/20...  Training Step: 972...  Training loss: 1.6443...  9.4417 sec/batch\n",
      "Epoch: 5/20...  Training Step: 973...  Training loss: 1.6457...  9.1135 sec/batch\n",
      "Epoch: 5/20...  Training Step: 974...  Training loss: 1.6681...  9.8650 sec/batch\n",
      "Epoch: 5/20...  Training Step: 975...  Training loss: 1.6621...  8.9944 sec/batch\n",
      "Epoch: 5/20...  Training Step: 976...  Training loss: 1.6770...  9.0464 sec/batch\n",
      "Epoch: 5/20...  Training Step: 977...  Training loss: 1.6627...  8.9048 sec/batch\n",
      "Epoch: 5/20...  Training Step: 978...  Training loss: 1.6499...  9.6999 sec/batch\n",
      "Epoch: 5/20...  Training Step: 979...  Training loss: 1.6705...  9.7239 sec/batch\n",
      "Epoch: 5/20...  Training Step: 980...  Training loss: 1.6493...  9.1220 sec/batch\n",
      "Epoch: 5/20...  Training Step: 981...  Training loss: 1.6615...  9.8850 sec/batch\n",
      "Epoch: 5/20...  Training Step: 982...  Training loss: 1.6672...  9.4097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 983...  Training loss: 1.6514...  9.3847 sec/batch\n",
      "Epoch: 5/20...  Training Step: 984...  Training loss: 1.6394...  9.6313 sec/batch\n",
      "Epoch: 5/20...  Training Step: 985...  Training loss: 1.6664...  9.2115 sec/batch\n",
      "Epoch: 5/20...  Training Step: 986...  Training loss: 1.6421...  9.7259 sec/batch\n",
      "Epoch: 5/20...  Training Step: 987...  Training loss: 1.6279...  8.5961 sec/batch\n",
      "Epoch: 5/20...  Training Step: 988...  Training loss: 1.6635...  9.6489 sec/batch\n",
      "Epoch: 5/20...  Training Step: 989...  Training loss: 1.6435...  9.5308 sec/batch\n",
      "Epoch: 5/20...  Training Step: 990...  Training loss: 1.6429...  11.0853 sec/batch\n",
      "Epoch: 6/20...  Training Step: 991...  Training loss: 1.7519...  9.6694 sec/batch\n",
      "Epoch: 6/20...  Training Step: 992...  Training loss: 1.6541...  9.8690 sec/batch\n",
      "Epoch: 6/20...  Training Step: 993...  Training loss: 1.6416...  9.6559 sec/batch\n",
      "Epoch: 6/20...  Training Step: 994...  Training loss: 1.6459...  9.9431 sec/batch\n",
      "Epoch: 6/20...  Training Step: 995...  Training loss: 1.6473...  10.9137 sec/batch\n",
      "Epoch: 6/20...  Training Step: 996...  Training loss: 1.6025...  10.0187 sec/batch\n",
      "Epoch: 6/20...  Training Step: 997...  Training loss: 1.6483...  9.9491 sec/batch\n",
      "Epoch: 6/20...  Training Step: 998...  Training loss: 1.6374...  10.7006 sec/batch\n",
      "Epoch: 6/20...  Training Step: 999...  Training loss: 1.6646...  9.8560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.6401...  10.9968 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1001...  Training loss: 1.6199...  10.9563 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1002...  Training loss: 1.6217...  10.3443 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1003...  Training loss: 1.6408...  9.6539 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1004...  Training loss: 1.6874...  11.1704 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1005...  Training loss: 1.6285...  10.2153 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1006...  Training loss: 1.6241...  10.8647 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1007...  Training loss: 1.6494...  9.2881 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1008...  Training loss: 1.6834...  9.2676 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1009...  Training loss: 1.6533...  9.8770 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1010...  Training loss: 1.6812...  9.9613 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1011...  Training loss: 1.6401...  10.6045 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1012...  Training loss: 1.6724...  9.2516 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1013...  Training loss: 1.6458...  8.9519 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1014...  Training loss: 1.6469...  8.3950 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1015...  Training loss: 1.6556...  8.4840 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1016...  Training loss: 1.6167...  8.7029 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1017...  Training loss: 1.6116...  11.0116 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1018...  Training loss: 1.6575...  8.4400 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1019...  Training loss: 1.6723...  8.7312 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1020...  Training loss: 1.6645...  8.7112 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1021...  Training loss: 1.6467...  8.3349 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1022...  Training loss: 1.6182...  8.2969 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1023...  Training loss: 1.6666...  8.5486 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1024...  Training loss: 1.6601...  8.4990 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1025...  Training loss: 1.6318...  8.3019 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1026...  Training loss: 1.6420...  8.3640 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1027...  Training loss: 1.6227...  8.6156 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1028...  Training loss: 1.6040...  8.3039 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1029...  Training loss: 1.5969...  8.3609 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1030...  Training loss: 1.6147...  8.8973 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1031...  Training loss: 1.6179...  8.2959 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1032...  Training loss: 1.6587...  8.2879 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1033...  Training loss: 1.6119...  8.3549 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1034...  Training loss: 1.6064...  8.5061 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1035...  Training loss: 1.6498...  8.4000 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1036...  Training loss: 1.5871...  8.4760 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1037...  Training loss: 1.6249...  11.1669 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1038...  Training loss: 1.6200...  9.9070 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1039...  Training loss: 1.6165...  10.4496 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1040...  Training loss: 1.6587...  11.2265 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1041...  Training loss: 1.6117...  11.0601 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1042...  Training loss: 1.6896...  10.1248 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1043...  Training loss: 1.6326...  9.0204 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1044...  Training loss: 1.6384...  10.0289 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1045...  Training loss: 1.6244...  12.2227 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1046...  Training loss: 1.6353...  10.4724 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1047...  Training loss: 1.6611...  10.2753 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1048...  Training loss: 1.6154...  9.8480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1049...  Training loss: 1.6122...  8.9806 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.6674...  10.5915 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1051...  Training loss: 1.6408...  8.1098 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1052...  Training loss: 1.6861...  8.0567 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1053...  Training loss: 1.6628...  9.3887 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1054...  Training loss: 1.6380...  8.1268 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1055...  Training loss: 1.6250...  7.9672 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1056...  Training loss: 1.6392...  8.1028 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1057...  Training loss: 1.6408...  7.9817 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1058...  Training loss: 1.6017...  7.9206 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1059...  Training loss: 1.6406...  7.9997 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1060...  Training loss: 1.6172...  9.2776 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1061...  Training loss: 1.6750...  8.5190 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1062...  Training loss: 1.6614...  9.3747 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1063...  Training loss: 1.6653...  10.1371 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1064...  Training loss: 1.6305...  10.0712 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1065...  Training loss: 1.6255...  8.7237 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1066...  Training loss: 1.6639...  8.0617 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1067...  Training loss: 1.6265...  8.2929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1068...  Training loss: 1.6365...  8.1178 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1069...  Training loss: 1.5837...  8.1288 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1070...  Training loss: 1.6310...  7.9957 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1071...  Training loss: 1.5904...  8.9744 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1072...  Training loss: 1.6343...  9.2786 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1073...  Training loss: 1.5888...  9.2711 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1074...  Training loss: 1.6324...  10.7357 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1075...  Training loss: 1.5882...  23.4030 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1076...  Training loss: 1.6021...  8.9303 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1077...  Training loss: 1.5927...  8.4108 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1078...  Training loss: 1.5945...  8.1138 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1079...  Training loss: 1.5869...  8.1974 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1080...  Training loss: 1.6290...  8.1238 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1081...  Training loss: 1.5858...  8.0893 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1082...  Training loss: 1.6072...  8.1868 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1083...  Training loss: 1.5858...  8.3099 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1084...  Training loss: 1.5975...  8.2078 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1085...  Training loss: 1.5885...  9.8790 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1086...  Training loss: 1.6293...  7.6204 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1087...  Training loss: 1.6069...  8.0697 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1088...  Training loss: 1.5784...  8.1213 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1089...  Training loss: 1.6053...  8.0492 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1090...  Training loss: 1.5749...  8.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1091...  Training loss: 1.6113...  8.2899 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1092...  Training loss: 1.6083...  8.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1093...  Training loss: 1.5971...  8.2128 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1094...  Training loss: 1.6047...  8.1108 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1095...  Training loss: 1.5941...  8.0187 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1096...  Training loss: 1.6103...  8.1033 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1097...  Training loss: 1.6091...  8.0917 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1098...  Training loss: 1.6119...  8.2108 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1099...  Training loss: 1.6120...  8.0427 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.6276...  8.0337 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1101...  Training loss: 1.6022...  8.2439 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1102...  Training loss: 1.6051...  9.6499 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-38b55e3a67b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m                                                  \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                                                  model.optimizer], \n\u001b[1;32m---> 30\u001b[1;33m                                                  feed_dict=feed)\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\l\\anaconda3\\envs\\aind-cv1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\l\\anaconda3\\envs\\aind-cv1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\l\\anaconda3\\envs\\aind-cv1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\l\\anaconda3\\envs\\aind-cv1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\l\\anaconda3\\envs\\aind-cv1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
